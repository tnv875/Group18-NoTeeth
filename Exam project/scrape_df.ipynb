{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scraping_class\n",
    "logfile = 'log.txt'## name your log file.\n",
    "connector = scraping_class.Connector(logfile)\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import html5lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df(url):\n",
    "    # Requesting URL with get\n",
    "    response = requests.get(url) \n",
    "    \n",
    "    # Getting course_id and year direct from URL\n",
    "    course_id = url.split('/')[-1]\n",
    "    year = url.split('/')[-3]\n",
    "\n",
    "    # Converting to soup\n",
    "    raw_html = response.text\n",
    "    soup = BeautifulSoup(raw_html,'lxml')\n",
    "\n",
    "    # Info used to scrabe certain tables (exam and on in the right corner)\n",
    "    info = soup.find_all(\"dl\", {'class':'dl-horizontal'})\n",
    "    \n",
    "    # Generating error list if course page is out of match\n",
    "    error = []\n",
    "\n",
    "    # Only selecting courses with 2 tabels i.e. exactly 1 exam and the info bar to the right\n",
    "    # Then appending information of each side by 'dt' and 'dd' and pairing them\n",
    "    if len(info) == 2:\n",
    "        comp_info = pd.DataFrame()\n",
    "        cleaned_id_text = ['Course Name']\n",
    "        for i in info[0].find_all('dt'):\n",
    "            cleaned_id_text.append(i.text)\n",
    "        cleaned_id__attrb_text = [soup.find('h1',{'class':\"courseTitle\"}).text]\n",
    "        for i in info[0].find_all('dd'):\n",
    "            cleaned_id__attrb_text.append(i.text)\n",
    "        \n",
    "        # If there is a blank space the lists can't be paired. Therfore if lengths dosen't match, list will be created by siblings if sibling isen't on the initial side.\n",
    "        if len(info[1].find_all('dt')) == len(info[1].find_all('dd')):\n",
    "            for i in info[1].find_all('dt'):\n",
    "                cleaned_id_text.append(i.text)\n",
    "            for i in info[1].find_all('dd'):\n",
    "                cleaned_id__attrb_text.append(i.text)\n",
    "        else:\n",
    "            for i in info[1].find_all('dt'):\n",
    "                cleaned_id_text.append(i.text)\n",
    "            for i in info[1].find_all('dt'):\n",
    "                if i.next_sibling in info[1].find_all('dt'):\n",
    "                    cleaned_id__attrb_text.append('')\n",
    "                else:\n",
    "                    cleaned_id__attrb_text.append(i.next_sibling.text)\n",
    "\n",
    "        # Appending year from URL\n",
    "        cleaned_id_text.append('Year')\n",
    "        cleaned_id__attrb_text.append(year)\n",
    "        \n",
    "        # Appending course id form URL\n",
    "        cleaned_id_text.append('Course id')\n",
    "        cleaned_id__attrb_text.append(course_id)\n",
    "        \n",
    "        # Scrabing data from workload table\n",
    "        work = soup.find('ul',{'class':'list-unstyled workload clearfix'})\n",
    "        workfi = work.find_all('li')\n",
    "        \n",
    "        # Geting 1 list of element, therfore has to be paire 1-3 2-4 etc.\n",
    "        for i in range(len(workfi)):\n",
    "            if i % 2 ==0:\n",
    "                cleaned_id_text.append(workfi[i].text)\n",
    "            else: \n",
    "                cleaned_id__attrb_text.append(workfi[i].text)\n",
    "        \n",
    "        # Appending content\n",
    "        cleaned_id_text.append(soup.find_all('div',{'class':\"course-content\"})[0].find('a').text)\n",
    "        cleaned_id__attrb_text.append(soup.find_all('div',{'class':\"course-content\"})[0].find('div').text)\n",
    "        \n",
    "        # Appending learning outcome\n",
    "        cleaned_id_text.append(soup.find_all('div',{'class':\"course-description\"})[0].find('a').text)\n",
    "        cleaned_id__attrb_text.append(soup.find_all('div',{'class':\"course-description\"})[0].find('div').text)\n",
    "\n",
    "        comp_info['Id'] = cleaned_id_text\n",
    "        comp_info['Attribute'] = cleaned_id__attrb_text\n",
    "    else:\n",
    "        error.append(url)\n",
    "        comp_info = []\n",
    "        \n",
    "    return(comp_info, error)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
